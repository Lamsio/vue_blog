(window.webpackJsonp=window.webpackJsonp||[]).push([[65],{391:function(s,a,t){"use strict";t.r(a);var n=t(4),r=Object(n.a)({},(function(){var s=this,a=s._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h4",{attrs:{id:"数据无量纲化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据无量纲化"}},[s._v("#")]),s._v(" 数据无量纲化")]),s._v(" "),a("p",[s._v("在机器学习算法实践中，我们往往有着将不同规格的数据转换到同一规格，或不同分布的数据转换到相同分布的需求，这种需求统称为将数据“无量纲化”，比方说希望数据都在0~10之间亦或是数据都满足标准正态分布。在比如逻辑回归、支持向量机、神经网络中，无量纲化能够加快求解速度。而在距离类模型，警如K近邻，K-Means聚类中，无量纲化可以帮我们提升模型精度，避免某一个取值范围特别大的特征对距离计算造成影响。(一个特例是决策树和树的集成算法们，对决策树我们不需要无量纲化，决策树可以把任意数据都处理得很好。)")]),s._v(" "),a("p",[s._v("数据的无量纲化可以是线性的，也可以是非线性的。线性的无量纲化包括中心化 (Zero-centered或者Mean-subtraction)处理和缩放处理(Scale)。中心化的本质是让所有记录减去一个固定值，即让数据样本数据平移到某个位置。缩放的本质是通过除以一个固定值，将数据固定在某个范围之中，取对数也算是一种缩放处理。")]),s._v(" "),a("h6",{attrs:{id:"preprocessing-minmaxscaler"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#preprocessing-minmaxscaler"}},[s._v("#")]),s._v(" preprocessing.MinMaxScaler")]),s._v(" "),a("p",[s._v("当数据 (X) 按照最小值中心化后，再按极差(最大值 - 最小值)缩放，数据移动了最小值个单位，并且会被收敛到[0,1]之间，而这个过程，就叫做数据归一化("),a("code",[s._v("Normalization")]),s._v("，又称"),a("code",[s._v("Min-Max Scaling")]),s._v(")。注意，"),a("code",[s._v("Normalization")]),s._v("是归一化，不是正则化，真正的正则化是"),a("code",[s._v("Regularization")]),s._v("，不是数据预处理的一种手段。归一化之后的数据服从正态分布，公式为 "),a("code",[s._v("new_x = (x - min(x)) / max(x) - min(x)")])]),s._v(" "),a("p",[s._v("在sklearn中，我们使用"),a("code",[s._v("preprocessing.MinMaxScaler")]),s._v("实现该功能。"),a("code",[s._v("MinMaxScalar")]),s._v("中有个重要参数"),a("code",[s._v("feature_range")]),s._v("，用于空置我们希望把数据压缩到的范围，默认为[0,1]")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("preprocessing "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MinMaxScaler\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" np\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" pandas "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" pd\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" matplotlib"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("pyplot "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" plt\n\ndata "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("6")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("18")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\npd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("DataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nscaler "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" MinMaxScaler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nresult "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" scaler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit_transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nresult\n\n"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''\narray([[0.  , 0.  ],\n       [0.25, 0.25],\n       [0.5 , 0.5 ],\n       [1.  , 1.  ]])\n'''")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br")])]),a("h6",{attrs:{id:"preprocessing-standardscaler"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#preprocessing-standardscaler"}},[s._v("#")]),s._v(" preprocessing.StandardScaler")]),s._v(" "),a("p",[s._v("当数据（x）按均值（μ）中心化后，再按标准差θ缩放，数据就会服从为均值为0，方差为1的正态分布，这个过程叫做数据标准化（Standardization），公式为"),a("code",[s._v("new_x = (x - μ)/θ")])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("preprocessing "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" StandardScaler\ndata "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("6")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("18")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\nscaler "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" StandardScaler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nscaler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nscaler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("mean_ "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#均值")]),s._v("\nscaler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("var_ "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#方差")]),s._v("\nresult "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" scaler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nresult\n\n"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''\narray([[-1.18321596, -1.18321596],\n       [-0.50709255, -0.50709255],\n       [ 0.16903085,  0.16903085],\n       [ 1.52127766,  1.52127766]])\n'''")]),s._v("\n\nresult"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#均值为0")]),s._v("\nresult"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("std"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#标准差为1")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br")])]),a("h4",{attrs:{id:"我该选择哪一种"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#我该选择哪一种"}},[s._v("#")]),s._v(" 我该选择哪一种？")]),s._v(" "),a("p",[s._v("这要视乎情况决定，大多数机器学习算法中，我们会选择StandardScaler进行特征缩放，因为MinMaxScaler对异常值极度敏感！假设数据普遍在0~10之间，而出现一个1000的异常值，该异常值会被捕获成为最大值，这将极大地影响最终结果。")]),s._v(" "),a("p",[s._v("MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比方说数字图像处理中的量化像素强度时，都会使用该方法将数据压缩到0~1之间")]),s._v(" "),a("p",[s._v("总而言之，我们会优先选择StandardScaler，效果不好时再选MinMaxScaler。")]),s._v(" "),a("p",[s._v("此外，还有别的缩放选择，例如MaxAbsScaler，当你希望压缩数据时，不影响矩阵中取值为0的个数时，可以考虑该缩放器")]),s._v(" "),a("h2",{attrs:{id:"处理分类型数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#处理分类型数据"}},[s._v("#")]),s._v(" 处理分类型数据")]),s._v(" "),a("h6",{attrs:{id:"preprocessing-labelencoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#preprocessing-labelencoder"}},[s._v("#")]),s._v(" preprocessing.LabelEncoder")]),s._v(" "),a("p",[s._v("该编码器用于将文本分类转化为分类数值")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("preprocessing "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" LabelEncoder\ny "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"OWN_OCCUPIED"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\nle "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" LabelEncoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nlabel "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" le"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit_transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nlabel\n\n"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''\ndata[\"OWN_OCCUPIED\"]\n0    Y\n1    N\n2    N\n3    C\n4    Y\n5    Y\n6    Y\n7    Y\n8    Y\n\nlabel:\narray([2, 1, 1, 0, 2, 2, 2, 2, 2])\n'''")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br")])]),a("p",[s._v("上述是基于一维特征进行编码，如果是多维特征需要用到OrdinalEncoder")]),s._v(" "),a("h6",{attrs:{id:"onehotencoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#onehotencoder"}},[s._v("#")]),s._v(" OneHotEncoder")]),s._v(" "),a("p",[s._v("分类数据的性质可能不同，例如舱门（S,C,Q）彼此毫无联系，而体重（>45kg , >90kg , >135kg）则不同，各个取值之间是有联系的，且是可以互相计算的，例如120-45 = 90，这是有距变量")]),s._v(" "),a("p",[s._v("然而在对特征进行编码的时候，这三种分类数据都会被我们转换为[0,1,2]，这三个数字在算法看来，是连续且可以计算的，这三个数字相互不等，有大小，并且有着可以相加相乘的联系。所以算法会把舱门，学历这样的分类特征，都误会成是体重这样的分类特征。这是说，我们把分类转换成数字的时候，忽略了数字中自带的数学性质，所以给算法传达了一些不准确的信息，而这会影响我们的建模")]),s._v(" "),a("p",[s._v('在此，我们要引入一个新概念 - 哑变量\n即原本，"S","Q","C" => [0,1,2]\n但现在,   "S","Q","C" => [[1,0,0],[0,1,0],[0,0,1]]\n通过这种高维形式能让三种类别相互独立存在')]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("preprocessing "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" OneHotEncoder\ny "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"OWN_OCCUPIED"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nohe "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" OneHotEncoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nohe "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" ohe"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nohe"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get_feature_names_out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 查看对应的标签")]),s._v("\npd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("DataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("ohe"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("toarray"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("h2",{attrs:{id:"处理连续性数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#处理连续性数据"}},[s._v("#")]),s._v(" 处理连续性数据")]),s._v(" "),a("h4",{attrs:{id:"skleran-preprocessing-binarizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#skleran-preprocessing-binarizer"}},[s._v("#")]),s._v(" skleran.preprocessing.Binarizer")]),s._v(" "),a("p",[s._v("根据阈值将数据二值化（将特征值设置为0或者1），用于处理连续型变量，大于阈值的值映射为1，反之为0。默认阈值为0时，特征中所有正值豆映射到1。")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("preprocessing "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Binarizer\ndata_2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("copy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nx "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" data_2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("iloc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nb "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" Binarizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("threshold"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("200")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nb "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit_transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])])])}),[],!1,null,null,null);a.default=r.exports}}]);